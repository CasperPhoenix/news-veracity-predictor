{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\n\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.layers import Input, Dense, Dropout, Embedding\nfrom sklearn.model_selection import train_test_split \nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.optimizers import Adam\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report, f1_score\nfrom transformers import AutoTokenizer, TFBertModel\n\nSEED = 10","metadata":{"execution":{"iopub.status.busy":"2023-03-31T09:25:37.713639Z","iopub.execute_input":"2023-03-31T09:25:37.713912Z","iopub.status.idle":"2023-03-31T09:25:48.641616Z","shell.execute_reply.started":"2023-03-31T09:25:37.713877Z","shell.execute_reply":"2023-03-31T09:25:48.640542Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/fake-news-classification/WELFake_Dataset.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-03-31T09:25:53.251099Z","iopub.execute_input":"2023-03-31T09:25:53.252188Z","iopub.status.idle":"2023-03-31T09:25:58.609154Z","shell.execute_reply.started":"2023-03-31T09:25:53.252146Z","shell.execute_reply":"2023-03-31T09:25:58.608142Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"df.dropna(subset = ['text','title'],inplace=True)\ndf['combined'] = df['title']+\" \"+df['text']\ndf.drop(['Unnamed: 0','title','text'],axis=1,inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-03-31T09:26:15.516578Z","iopub.execute_input":"2023-03-31T09:26:15.517280Z","iopub.status.idle":"2023-03-31T09:26:15.929383Z","shell.execute_reply.started":"2023-03-31T09:26:15.517240Z","shell.execute_reply":"2023-03-31T09:26:15.928377Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"df.drop_duplicates(inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-03-31T09:26:21.783176Z","iopub.execute_input":"2023-03-31T09:26:21.783880Z","iopub.status.idle":"2023-03-31T09:26:22.483889Z","shell.execute_reply.started":"2023-03-31T09:26:21.783839Z","shell.execute_reply":"2023-03-31T09:26:22.482843Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"X = df['combined']\ny = df['label']","metadata":{"execution":{"iopub.status.busy":"2023-03-31T09:26:34.996510Z","iopub.execute_input":"2023-03-31T09:26:34.996928Z","iopub.status.idle":"2023-03-31T09:26:35.005548Z","shell.execute_reply.started":"2023-03-31T09:26:34.996892Z","shell.execute_reply":"2023-03-31T09:26:35.002527Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"MAX_LEN = 128\n\ndef get_model():\n    dropout_rate = 0.16\n\n    input_ids = Input(shape = (MAX_LEN,), dtype = tf.int32, name = 'input_ids')\n    input_mask = Input(shape = (MAX_LEN,), dtype = tf.int32, name = 'input_mask')\n\n    embeddings = bert([input_ids, input_mask])[1] #pooler output\n    print(embeddings)\n\n    out = Dropout(dropout_rate)(embeddings)\n    \n    #64 units dense layer\n    out = Dense(64,activation = 'relu')(out)\n    out = Dropout(dropout_rate)(out)\n\n    y = Dense(1,activation = 'sigmoid')(out)\n    \n    model = Model(inputs=[input_ids, input_mask], outputs=y)\n    model.layers[2].trainable = True\n    \n    #define optimizer\n    optimizer = Adam(learning_rate=1e-05, epsilon=1e-08,clipnorm=1.0)\n    \n    #complile the model\n    model.compile(optimizer = optimizer, loss = 'binary_crossentropy', metrics = 'accuracy')\n    \n    return model\n\n#define tokenization function\ndef get_tokens(X):\n    \n    X = tokenizer(\n                text = list(X),\n                add_special_tokens = True,\n                max_length = MAX_LEN,\n                truncation = True,\n                padding = True,\n                return_tensors = 'tf',\n                return_token_type_ids = False,\n                return_attention_mask = True,\n                verbose = True\n                )\n    \n    return X","metadata":{"execution":{"iopub.status.busy":"2023-03-31T09:26:50.534561Z","iopub.execute_input":"2023-03-31T09:26:50.534928Z","iopub.status.idle":"2023-03-31T09:26:50.547231Z","shell.execute_reply.started":"2023-03-31T09:26:50.534894Z","shell.execute_reply":"2023-03-31T09:26:50.546151Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\nbert = TFBertModel.from_pretrained('bert-base-uncased')\n\nmodel = get_model()\n","metadata":{"execution":{"iopub.status.busy":"2023-03-31T09:27:01.744010Z","iopub.execute_input":"2023-03-31T09:27:01.744506Z","iopub.status.idle":"2023-03-31T09:27:21.973645Z","shell.execute_reply.started":"2023-03-31T09:27:01.744423Z","shell.execute_reply":"2023-03-31T09:27:21.972642Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a8a3ca9d95d74c40b7594dbd082561bd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c582244785254806903b4d7acf970436"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5bf30c32478f4dfd9edcf6167681fb73"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"88f14e94145547d2a4c2edd1cde6250d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading tf_model.h5:   0%|          | 0.00/536M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f36eb6ffe02f4c32bba6c3036c4302fb"}},"metadata":{}},{"name":"stderr","text":"Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nAll the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n","output_type":"stream"},{"name":"stdout","text":"KerasTensor(type_spec=TensorSpec(shape=(None, 768), dtype=tf.float32, name=None), name='tf_bert_model/bert/pooler/dense/Tanh:0', description=\"created by layer 'tf_bert_model'\")\n","output_type":"stream"}]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2023-03-31T09:27:21.975664Z","iopub.execute_input":"2023-03-31T09:27:21.976036Z","iopub.status.idle":"2023-03-31T09:27:22.031103Z","shell.execute_reply.started":"2023-03-31T09:27:21.975992Z","shell.execute_reply":"2023-03-31T09:27:22.030247Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Model: \"model\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n input_ids (InputLayer)         [(None, 128)]        0           []                               \n                                                                                                  \n input_mask (InputLayer)        [(None, 128)]        0           []                               \n                                                                                                  \n tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_ids[0][0]',              \n                                thPoolingAndCrossAt               'input_mask[0][0]']             \n                                tentions(last_hidde                                               \n                                n_state=(None, 128,                                               \n                                 768),                                                            \n                                 pooler_output=(Non                                               \n                                e, 768),                                                          \n                                 past_key_values=No                                               \n                                ne, hidden_states=N                                               \n                                one, attentions=Non                                               \n                                e, cross_attentions                                               \n                                =None)                                                            \n                                                                                                  \n dropout_37 (Dropout)           (None, 768)          0           ['tf_bert_model[0][1]']          \n                                                                                                  \n dense (Dense)                  (None, 64)           49216       ['dropout_37[0][0]']             \n                                                                                                  \n dropout_38 (Dropout)           (None, 64)           0           ['dense[0][0]']                  \n                                                                                                  \n dense_1 (Dense)                (None, 1)            65          ['dropout_38[0][0]']             \n                                                                                                  \n==================================================================================================\nTotal params: 109,531,521\nTrainable params: 109,531,521\nNon-trainable params: 0\n__________________________________________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, test_size = 0.2, random_state = SEED,shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2023-03-31T09:27:33.044059Z","iopub.execute_input":"2023-03-31T09:27:33.044534Z","iopub.status.idle":"2023-03-31T09:27:33.099041Z","shell.execute_reply.started":"2023-03-31T09:27:33.044487Z","shell.execute_reply":"2023-03-31T09:27:33.097920Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"X_train_seq = get_tokens(X_train)\nX_test_seq = get_tokens(X_test)\n\nhistory = model.fit(x = {'input_ids':X_train_seq['input_ids'],'input_mask':X_train_seq['attention_mask']}, \n                    y = y_train, epochs=10,\n                    validation_split = 0.2, batch_size = 32, callbacks=[EarlyStopping(monitor='val_accuracy',mode='max', patience=3, verbose=False,restore_best_weights=True)])","metadata":{"execution":{"iopub.status.busy":"2023-03-31T09:27:46.581283Z","iopub.execute_input":"2023-03-31T09:27:46.582282Z","iopub.status.idle":"2023-03-31T11:24:12.147666Z","shell.execute_reply.started":"2023-03-31T09:27:46.582241Z","shell.execute_reply":"2023-03-31T11:24:12.146429Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Epoch 1/10\n1263/1263 [==============================] - 756s 560ms/step - loss: 0.0758 - accuracy: 0.9718 - val_loss: 0.0501 - val_accuracy: 0.9819\nEpoch 2/10\n1263/1263 [==============================] - 685s 543ms/step - loss: 0.0195 - accuracy: 0.9929 - val_loss: 0.0244 - val_accuracy: 0.9916\nEpoch 3/10\n1263/1263 [==============================] - 684s 541ms/step - loss: 0.0109 - accuracy: 0.9958 - val_loss: 0.0319 - val_accuracy: 0.9906\nEpoch 4/10\n1263/1263 [==============================] - 684s 542ms/step - loss: 0.0047 - accuracy: 0.9984 - val_loss: 0.0412 - val_accuracy: 0.9918\nEpoch 5/10\n1263/1263 [==============================] - 683s 541ms/step - loss: 0.0029 - accuracy: 0.9990 - val_loss: 0.0514 - val_accuracy: 0.9885\nEpoch 6/10\n1263/1263 [==============================] - 654s 518ms/step - loss: 0.0027 - accuracy: 0.9990 - val_loss: 0.0396 - val_accuracy: 0.9915\nEpoch 7/10\n1263/1263 [==============================] - 684s 541ms/step - loss: 0.0029 - accuracy: 0.9992 - val_loss: 0.0372 - val_accuracy: 0.9921\nEpoch 8/10\n1263/1263 [==============================] - 683s 541ms/step - loss: 0.0016 - accuracy: 0.9994 - val_loss: 0.0533 - val_accuracy: 0.9904\nEpoch 9/10\n1263/1263 [==============================] - 682s 540ms/step - loss: 0.0021 - accuracy: 0.9994 - val_loss: 0.0371 - val_accuracy: 0.9920\nEpoch 10/10\n1263/1263 [==============================] - 682s 540ms/step - loss: 0.0015 - accuracy: 0.9996 - val_loss: 0.0374 - val_accuracy: 0.9903\n","output_type":"stream"}]},{"cell_type":"code","source":"def plot_graphs(history, metric):\n    \n    plt.plot(history.history[metric])\n    plt.plot(history.history['val_'+metric], '')\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(metric)\n    plt.legend([metric, 'val_'+metric])\n\n\nplt.figure(figsize=(16, 6))\nplt.subplot(1, 2, 1)\nplot_graphs(history, 'accuracy')\nplt.subplot(1, 2, 2)\nplot_graphs(history, 'loss')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"yhat =  np.where(model.predict({'input_ids':X_test_seq['input_ids'],'input_mask':X_test_seq['attention_mask']}) >=0.5,1,0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_test,yhat))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (8,6))\n\nsns.heatmap(confusion_matrix(y_test,yhat), annot=True, \n            fmt='', cmap='Blues')\n\nplt.xlabel('Predicted Labels')\nplt.ylabel('Real Labels')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save('fake_news_bert.h5')","metadata":{"execution":{"iopub.status.busy":"2023-03-31T11:30:18.069174Z","iopub.execute_input":"2023-03-31T11:30:18.069911Z","iopub.status.idle":"2023-03-31T11:30:23.939350Z","shell.execute_reply.started":"2023-03-31T11:30:18.069870Z","shell.execute_reply":"2023-03-31T11:30:23.938288Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"model.save_weights(\"fake_news_bert_weights\")","metadata":{"execution":{"iopub.status.busy":"2023-03-31T11:34:45.274923Z","iopub.execute_input":"2023-03-31T11:34:45.275877Z"},"trusted":true},"execution_count":null,"outputs":[]}]}